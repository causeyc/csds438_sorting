\documentclass[conference]{IEEEtran}
%\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage[table]{xcolor}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{subcaption}
%\usepackage{subfig}
\usepackage{array}
\usepackage{multirow}
\usepackage{multicol}
\hypersetup{
	colorlinks = true,
	linkcolor = blue,
	anchorcolor = blue,
	citecolor = blue,
	filecolor = blue,
	urlcolor = blue
}	
\usepackage[ruled,vlined]{algorithm2e}
\newcommand\MyBox[2]{
	\fbox{\lower0.75cm
		\vbox to 1.7cm{\vfil
			\hbox to 1.7cm{\hfil\parbox{1.4cm}{#1\\#2}\hfil}
			\vfil}%
	}%
}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
		T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}
		\title{Comparing Parallel Sorting Algorithms on a High Performance Computing Cluster Using OpenMP}
			\author{\IEEEauthorblockN{Benjamin Pierce}
			\IEEEauthorblockA{
				\textit{Department of Computer and Data Sciences}\\ }
			\and
			\IEEEauthorblockN{Alberto Safra}
			\IEEEauthorblockA{ 
				\textit{Department of Computer and Data Sciences}}
			\and
			\IEEEauthorblockN{Colin Causey}
			\IEEEauthorblockA{ 
			\textit{Department of Computer and Data Sciences}}
			\and
			\IEEEauthorblockN{Jason Richards}
			\IEEEauthorblockA{
				\textit{Department of Computer and Data Sciences}}}
	\maketitle
	
\begin{abstract}
“Many computer scientists consider sorting to be the most fundamental problem in the study of algorithms” (Introduction to Algorithms, pg. 148).

Sorting is commonly viewed as the most fundamental problem in the study of algorithms. Some cited reasons for this are that a great many software applications use sorting for various reasons, and a great many algorithms use sorting as a subroutine \cite{cormen_introduction_2009}. 
Given its ubiquity, therefore, it is valuable to be able to solve the sorting problem efficiently. 
For this reason, many efficient sorting algorithms have been developed (e.g., mergesort, quicksort, heapsort). 
Given the asymptotic lower bound of $\Omega(nlog(n)$ for comparison-based sorting algorithms such as these, a natural route to take to achieve greater performance is parallel computing. 
In the interest of wanting to select the optimal sorting algorithm to run on a parallel computer, it is valuable to empirically compare the performance of various parallelized sorting algorithms. 
This is the aim of our research. 
We will conduct an empirical analysis and comparison of various parallel sorting algorithms. 
The criteria for evaluation will be (i) execution time, (ii) memory footprint, and (iii) scalability. 
The research will be conducted on Case Western Reserve’s high-performance computing (HPC) architecture. 
We will implement various parallel sorting algorithms and execute them with variously sized and randomly permuted input arrays. The execution times and memory footprints will be recorded for each run. 
Additionally, we will run the algorithms on a varying number of CPUs (e.g., one CPU, two CPUs, four CPUs) in order to assess their scalability. 
After these data have been collected, we will perform data analysis and use it to compare the various sorting algorithms. 
The comparison will facilitate making an informed choice about which sorting algorithm to use under various conditions (e.g., the number of CPUs available and the size of the input array).
\end{abstract}

\section{Introduction}
Sorting is a fundamental problem in many different fields. 
Any algorithm that depends on having some ordering to the data will likely deal with sorting in some form, and even simple tasks like "find the top $n$ examples" are accomplished using sorts. 
However, today's world of "Big Data" has led to an astronomical increase in the amount of computing power used to process it. 
As a response, the field of distributed computing has become more and more important. 
Distributed computing is a paradigm where computation is spread across many processors, working at the same time, rather then the usual sequential, single-core processing method. 
In this fashion, parallelized versions of different sorting algorithms have emerged. 
This study focuses on four of the most popular sorting algorithms: Quicksort, Mergesort, Heapsort, and Insertion sort. 

\section{Background \& Theory}
As mentioned before, all comparison based sorts have an asymptotic lower bound of $\Omega(nlog(n)$. 
Indeed,  Mergesort, and Heapsort achieve a $O(nlog(n))$ upper bound as well, and Quicksort behaves as $O(nlog(n))$ in the average case. 
Insertion sort, as a less advanced algorithm, has a upper bound of $O(n^2)$, as does Quicksort. \cite{cormen_introduction_2009} 
However, Quicksort's upper bound is rarely the case, and tends to have smaller constant factors then either Mergesort or Heapsort. \cite{hoare_algorithm_1961} % TODO wrong cite
Additionally, each algorithm is constructed differently, and some are more amendable to parallelization then others. 
\subsection{Insertion sort}
Insertion sort is the simplest sorting algorithm discussed here. 
It works by iteratively building a sorted array one element at a time. 
That is, the algorithm loops through the list, creating a sorted and unsorted part, and insterting each element it encounters at the correct location. 
This proceduce is $O(n^2)$ worst case, but tends to perform quite well on small datasets, and better then other $O(n^2)$ sorts, such as bubble sort. \cite{knuth_art_nodate} 
For this reason, inserstion sort is often used as a subroutine in hybrid sorts like Timsort, where it is called when the size of a subarray is smaller then some threshold. \cite{auger_et_al:LIPIcs:2018:9467} 
Insertion sort can be parallelized % TODO in what way @Albert
\subsection{Mergesort}
Mergesort is a divide and conquer sort that recursively divides an array into subarrays, and merges them together such that each subarray is sorted. 
This procedure can be seen in Figure \ref{mrg}.  
The recursive invariant is that each returned subarray is sorted, which can be proved by induction, as an array of length 1 is already sorted by definition. \cite{cormen_introduction_2009} 
\begin{figure}[h]
	\includegraphics[width=6cm]{merge.png} 
	\caption{Mergesort diagram from \cite{cormen_introduction_2009}}
	\label{mrg}
\end{figure}
Mergesort has a time complexity of $O(nlog(n))$ in both the average and worst case, although constant factors can make it worse than Quicksort in practice. 
Mergesort is a natural candidate for parallelization; the divide and conquer nature of the algorithm is already suitable for this process. 
% TODO @Colin add detail on your implementation

\subsection{Heapsort}
Heapsort is another $O(n log(n))$ comparison sort. 
It, along with the heap data structure, was invented in 1964. \cite{forsythe_algorithms_1964}
Heapsort first turns the dataset into a max heap, which is a binary tree where each parent node is greater then its children. 
This process, called \textit{heapification}, is an $O(n)$ algorithum. 
Sorting is performed by repeatably popping the root node (the maximum value) and re-heapifying. 
This procedure takes advantage of the binary tree structure, and is worst case $O(n log (n))$ overall.  \cite{cormen_introduction_2009}
Unfortunately, Heapsort is a poor candidate for paralleization, as it does not partition into subarrays and depends on the root node being the absolute maximum. 

\subsection{Quicksort}
The final sort discussed here is Quicksort, another $O(n log(n))$ comparison sort. 
Developed in 1961 \cite{hoare_algorithm_1961}, Quicksort is a partitioning sort that works by selecting a pivot element in an array and partitioning based on a comparison to the pivot, as shown in Figure \ref{qck}. 
\begin{figure}[h]
	\includegraphics[width=6cm]{Quicksort.png} 
	\caption{Quicksort diagram. \href{https://www.techiedelight.com/quicksort/}{Source}}
	\label{qck}
\end{figure}
In particular, Quicksort is perhaps the most parallelizable sorting algorithm, and can achieve a linear speedup with few modifications. \cite{blelloch_programming_1996}
This is because each subarray can be sorted independently, and this leads to speedup. 
% TODO @Jason add detail on your implementation
\section{Methodology}
In this study, the OpenMP \cite{openmp08} API is used for parallization. 
It is an example of a fork-join methodology	, where each parallel thread forks off from a main thread, then, the results are joined back together. 
This is implemented into the C programming language via \textit{\#pragma} preprocessor directives. 
OpenMP is useful for obviously parallel cases, such as Mergesort and Quicksort, as the division of work is quite clear. 
% TODO add more OMP detail here.

This work utilizes Case Western Reserve University's Markov cluster, which runs on Intel Xeon x86\_64 Processors. 
Using the Simple Linux Utility for Resource Management \cite{yoo_slurm_2003} (SLURM), resources are allocated in batch mode, which enables repeatable, large scale experiments with the requested resources. 
% TODO do we need more detail?

\section{Results}
In this section, the results of each algorithm will be shown. 
The main variable of interest is how much time each algorithm takes as a function of the length of the input array $n$. 
We begin with an individual discussion of Quicksort. 

\subsection{Quicksort}
There was great success with parallel Quicksort. 
As a divide-and-conquer algorithm, it is a naturally parallel algorithum. 
The results of this algorithum can be seen in Figure \ref{qck_per}. 
\begin{figure}[h]
	\includegraphics[width=10.5cm]{qs_mt.png} 
	\caption{Performance of Quicksort}
	\label{qck_per}
\end{figure}

%TODO add discussions on quicksort performance

\subsection{Heapsort}
Heapsort proved to be purely sequential, as the core of the algorithm depends on universal array access in the current form. 
The results of the nonparallel Heapsort are seen in Figure \ref{hs_per}. 
\begin{figure}[h]
	\includegraphics[width=9cm]{hs_per.png} 
	\caption{Performance of Heapsort}
	\label{hs_per}
\end{figure}
As Figure \ref{hs_per} shows, the performance of Heapsort becomes quite poor very rapidly. 
Although in its current form, Heapsort cannot be parallelized, we present an alternative algorithm utilizing the core ideas of Heapsort in a parallel manner. 

\begin{algorithm}
	\SetAlgoLined
	\KwResult{A sorted list }
	list $a$\;
	list $result$\;
	int $threads$\;
	let $lists$ be $a$ partitioned into $threads$\\
	\While{every list in lists is not empty }{
		\textbf{In Parallel}\;
		\hspace{0.5cm}$heapify$ each list\;
		$pop$ the largest element of all sub-heaps into $result$\;
		$re-heapify$ the heap that has been popped\;
	}
	\Return $result$
	\caption{Parallel-Heaps}
	\label{alg}
\end{algorithm}
Algorithm \ref{alg} presents a possibly parallel algorithm that utilizes the idea of Heapsort. 
Essentially, the algorithm divides the array into many smaller heaps, each managed by a single thread. 
This way, all heapification can be done in parallel, and the "max" item is selected from the number of heaps through a straightforward traversal of the roots of the sub-heaps. 
However, this algorithum has several downsides. 
One is that the end reheapification is still sequential; as by necessity, one heap must be popped, the algorithm must wait on the reheapification of that heap. 
This heap will be smaller by a constant factor then the traditional, single-threaded Heapsort, however. 
Therefore, $Parallel-Heaps$ will be at most a constant speedup for Heapsort.

$Parallel-Heaps$ was not implemented in this study, as it is not possible to do with the OpenMP framework; such a method would require POSIX threads. 
For the sake of comparison, this algorithm was thus excluded. 
Further investigation of this $Parallel-Heaps$ algorithm is a topic for further study.  
\subsection{Mergesort}
As with Quicksort, Mergesort proved to be naturally parallel. 
The results of parallel Mergesort can be seen in Figure \ref{ms_per}
\begin{figure}[h]
	\includegraphics[width=9cm]{ms_per.png} 
	\caption{Performance of Mergesort}
	\label{ms_per}
\end{figure}
%TODO add mergesort implementation details



\subsection{Shellsort}
%TODO Do we have results for shellsort?


\subsection{Comparative Results}
Results are compared in a few different ways. 
One is to hold the number of threads constant, and plot the performance of each algorithm, as in Figure \ref{msqshs}, which shows that Heapsort, which is not paralleized, proves to be much worse then either Quicksort or Mergesort.
\begin{figure}
	\includegraphics[width=10.5cm]{mshsqs.png} 
	\caption{Comparative Performance with 8 threads}
	\label{msqshs}
\end{figure}
%TODO Add deets
Additionally, it is useful to compare results on a single array size with multiple threads. 
This can be seen for $10^6$ elements in Figure \ref{1e5}
\begin{figure}[h]
	\includegraphics[width=9cm]{1e5.png} 
	\caption{Comparative Performance with for $10^4$ elements}
	\label{1e5}
\end{figure}
Interestingly, the performance of Quicksort appears to worsen with the number of threads more then Mergesort; this suggests significant overhead introduced by parallelization. 
However, this effect goes away with an increase in the number of elements, as in Figure \ref{1e8}. 
\begin{figure}[h]
	\includegraphics[width=9cm]{1e8.png} 
	\caption{Comparative Performance with for $10^7$ elements}
	\label{1e8}
\end{figure}

This suggests that there is significant overhead with smaller array sizes, but this is overcome by efficeicny gains on larger array sizes. 
\section{Conclusion}
% TODO these are what we expect to see...
As expected, the three $O(n log(n))$ algorithms perform better then Insertion sort. 
Due to the more parallel nature of Quicksort and Mergesort, these algorithms benefit more from parallization then the more sequential Heapsort. 
This is because the divide and conquer strategy is inherently more parallel, which should be taken into account when developing new algorithms to be run on parallel and distributed computing platforms. 
% TODO further findings from implementation
\bibliography{ref.bib}
\bibliographystyle{ieeetran}
\appendix
% TODO slap the code in here
\end{document}